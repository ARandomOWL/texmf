
@Article{	  Ge2020,
  abstract	= {Hyperdimensional (HD) computing is built upon its unique
		  data type referred to as hypervectors. The dimension of
		  these hypervectors is typically in the range of tens of
		  thousands. Proposed to solve cognitive tasks, HD computing
		  aims at calculating similarity among its data. Data
		  transformation is realized by three operations, including
		  addition, multiplication and permutation. Its ultra-wide
		  data representation introduces redundancy against noise.
		  Since information is evenly distributed over every bit of
		  the hypervectors, HD computing is inherently robust.
		  Additionally, due to the nature of those three operations,
		  HD computing leads to fast learning ability, high energy
		  efficiency and acceptable accuracy in learning and
		  classification tasks. This paper introduces the background
		  of HD computing, and reviews the data representation, data
		  transformation, and similarity measurement. The
		  orthogonality in high dimensions presents opportunities for
		  flexible computing. To balance the tradeoff between
		  accuracy and efficiency, strategies include but are not
		  limited to encoding, retraining, binarization and hardware
		  acceleration. Evaluations indicate that HD computing shows
		  great potential in addressing problems using data in the
		  form of letters, signals and images. HD computing
		  especially shows significant promise to replace machine
		  learning algorithms as a light-weight classifier in the
		  field of internet of things (IoTs).},
  archiveprefix	= {arXiv},
  arxivid	= {2004.11204},
  author	= {Ge, Lulu and Parhi, Keshab K.},
  doi		= {10.1109/MCAS.2020.2988388},
  eprint	= {2004.11204},
  issn		= {15580830},
  journal	= {IEEE Circuits Syst. Mag.},
  month		= apr,
  number	= {2},
  pages		= {30--47},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{Classification Using Hyperdimensional Computing: A
		  Review}},
  volume	= {20},
  year		= {2020}
}

@Article{	  Karunaratne2019,
  abstract	= {Hyperdimensional computing (HDC) is an emerging computing
		  framework that takes inspiration from attributes of
		  neuronal circuits such as hyperdimensionality, fully
		  distributed holographic representation, and
		  (pseudo)randomness. When employed for machine learning
		  tasks such as learning and classification, HDC involves
		  manipulation and comparison of large patterns within
		  memory. Moreover, a key attribute of HDC is its robustness
		  to the imperfections associated with the computational
		  substrates on which it is implemented. It is therefore
		  particularly amenable to emerging non-von Neumann paradigms
		  such as in-memory computing, where the physical attributes
		  of nanoscale memristive devices are exploited to perform
		  computation in place. Here, we present a complete in-memory
		  HDC system that achieves a near-optimum trade-off between
		  design complexity and classification accuracy based on
		  three prototypical HDC related learning tasks, namely,
		  language classification, news classification, and hand
		  gesture recognition from electromyography signals.
		  Comparable accuracies to software implementations are
		  demonstrated, experimentally, using 760,000 phase-change
		  memory devices performing analog in-memory computing.},
  archiveprefix	= {arXiv},
  arxivid	= {1906.01548},
  author	= {Karunaratne, Geethan and Gallo, Manuel Le and Cherubini,
		  Giovanni and Benini, Luca and Rahimi, Abbas and Sebastian,
		  Abu},
  eprint	= {1906.01548},
  month		= jun,
  title		= {{In-memory hyperdimensional computing}},
  url		= {http://arxiv.org/abs/1906.01548},
  year		= {2019}
}
