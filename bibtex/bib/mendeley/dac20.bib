
@InProceedings{	  Ding2018a,
  abstract	= {Deep Neural Networks (DNNs) have been adopted in many
		  systems because of their higher classification accuracy,
		  with custom hardware implementations great candidates for
		  high-speed, accurate inference. While progress in achieving
		  large scale, highly accurate DNNs has been made,
		  significant energy and area are required due to massive
		  memory accesses and computations. Such demands pose a
		  challenge to any DNN implementation, yet it is more natural
		  to handle in a custom hardware platform. To alleviate the
		  increased demand in storage and energy, quantized DNNs
		  constrain their weights (and activations) from
		  floating-point numbers to only a few discrete levels.
		  Therefore, storage is reduced, thereby leading to less
		  memory accesses. In this paper, we provide an overview of
		  different types of quantized DNNs, as well as the training
		  approaches for them. Among the various quantized DNNs, our
		  LightNN (Light Neural Network) approach can reduce both
		  memory accesses and computation energy, by filling the gap
		  between classic, full-precision and binarized DNNs. We
		  provide a detailed comparison between LightNNs,
		  conventional DNNs and Binarized Neural Networks (BNNs),
		  with MNIST and CIFAR-10 datasets. In contrast to other
		  quantized DNNs that trade-off significant amounts of
		  accuracy for lower memory requirements, LightNNs can
		  significantly reduce storage, energy and area while still
		  maintaining a test error similar to a large DNN
		  configuration. Thus, LightNNs provide more options for
		  hardware designers to trade-off accuracy and energy.},
  author	= {Ding, Ruizhou and Liu, Zeye and Blanton, R. D.Shawn and
		  Marculescu, Diana},
  booktitle	= {Proc. Asia South Pacific Des. Autom. Conf. ASP-DAC},
  doi		= {10.1109/ASPDAC.2018.8297274},
  isbn		= {9781509006021},
  month		= feb,
  pages		= {1--8},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{Quantized deep neural networks for energy efficient
		  hardware-based inference}},
  volume	= {2018-Janua},
  year		= {2018}
}

@InProceedings{	  Goel2019,
  abstract	= {Recent research has focused on Deep Neural Networks (DNNs)
		  implemented directly in hardware. However, larger DNNs
		  require significant energy and area, thereby limiting their
		  wide adoption. We propose a novel DNN quantization
		  technique and a corresponding hardware solution, CompactNet
		  that optimizes the use of hardware resources even further,
		  through dynamic allocation of memory for each parameter.
		  Experimental results for the MNIST and CIFAR-10 datasets,
		  show that CompactNet reduces the memory requirement by over
		  80{\%}, the energy requirement by 12-fold, and the area
		  requirement by 7-fold, when compared to the conventional
		  DNN. This is achieved with minimal degradation to the
		  classification accuracy. We demonstrate that, CompactNet
		  provides pareto-optimal designs to make trade-offs between
		  accuracy and resource requirement. The applications of
		  CompactNet can be extended to datasets like ImageNet, and
		  into models like MobileNet.},
  author	= {Goel, Abhinav and Liu, Zeye and Blanton, Ronald D.},
  booktitle	= {Proc. - 2018 IEEE Int. Conf. Big Data, Big Data 2018},
  doi		= {10.1109/BigData.2018.8622329},
  isbn		= {9781538650356},
  keywords	= {ASIC,approximate computing,deep neural networks,low power
		  design},
  month		= jan,
  pages		= {4723--4729},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{CompactNet: High Accuracy Deep Neural Network Optimized
		  for On-Chip Implementation}},
  year		= {2019}
}

@Article{	  Hirtzlin2019,
  abstract	= {Binarized neural networks, a recently discovered class of
		  neural networks with minimal memory requirements and no
		  reliance on multiplication, are a fantastic opportunity for
		  the realization of compact and energy efficient inference
		  hardware. However, such neural networks are generally not
		  entirely binarized: their first layer remains with fixed
		  point input. In this paper, we propose a stochastic
		  computing version of binarized neural networks, where the
		  input is also binarized. The simulations on the example of
		  the Fashion-MNIST and CIFAR-10 datasets show that such
		  networks can approach the performance of conventional
		  binarized neural networks. We evidence that the training
		  procedure should be adapted for use with stochastic
		  computing. Finally, the ASIC implementation of our scheme
		  is investigated, in a system that closely associates logic
		  and memory, implemented by spin torque magnetoresistive
		  random access memory. This analysis shows that the
		  stochastic computing approach can allow considerable
		  savings with regards to conventional binarized neural
		  networks in terms of area (62{\%} area reduction on the
		  Fashion-MNIST task). It can also allow important savings in
		  terms of energy consumption if we accept a reasonable
		  reduction of accuracy: for example a factor 2.1 can be
		  saved, with the cost of 1.4{\%} in Fashion-MNIST test
		  accuracy. These results highlight the high potential of
		  binarized neural networks for hardware implementation, and
		  that adapting them to hardware constraints can provide
		  important benefits.},
  archiveprefix	= {arXiv},
  arxivid	= {1906.00915},
  author	= {Hirtzlin, Tifenn and Penkovsky, Bogdan and Bocquet, Marc
		  and Klein, Jacques Olivier and Portal, Jean Michel and
		  Querlioz, Damien},
  doi		= {10.1109/ACCESS.2019.2921104},
  eprint	= {1906.00915},
  issn		= {21693536},
  journal	= {IEEE Access},
  keywords	= {Binarized neural network,MRAM,embedded system,in memory
		  computing,stochastic computing},
  pages		= {76394--76403},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{Stochastic Computing for Hardware Implementation of
		  Binarized Neural Networks}},
  volume	= {7},
  year		= {2019}
}

@InProceedings{	  HokchhayTann2016,
  author	= {{Hokchhay Tann} and {Soheil Hashemi} and {R. Iris Bahar}
		  and {Sherief Reda}},
  title		= {{Runtime configurable deep neural networks for
		  energy-accuracy trade-off}},
  url		= {https://ieeexplore.ieee.org/document/7750982},
  year		= {2016}
}

@InProceedings{	  Jiang2018,
  abstract	= {We present an in-memory computing SRAM macro that computes
		  XNOR-and-accumulate in binary/ternary deep neural networks
		  on the bitline without row-by-row data access. It achieves
		  33X better energy and 300X better energy-delay product than
		  digital ASIC, and also achieves significantly higher
		  accuracy than prior in-SRAM computing macro (e.g., 98.3{\%}
		  vs. 90{\%} for MNIST) by being able to support the
		  mainstream DNN/CNN algorithms.},
  author	= {Jiang, Zhewei and Yin, Shihui and Seok, Mingoo and Seo,
		  Jae Sun},
  booktitle	= {Dig. Tech. Pap. - Symp. VLSI Technol.},
  doi		= {10.1109/VLSIT.2018.8510687},
  isbn		= {9781538642160},
  issn		= {07431562},
  month		= oct,
  pages		= {173--174},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{XNOR-SRAM: In-memory computing SRAM macro for
		  binary/ternary deep neural networks}},
  volume	= {2018-June},
  year		= {2018}
}

@InProceedings{	  Sun2018,
  abstract	= {Binary Neural Networks (BNNs) have been recently proposed
		  to improve the area-/energy-efficiency of the machine/deep
		  learning hardware accelerators, which opens an opportunity
		  to use the technologically more mature binary RRAM devices
		  to effectively implement the binary synaptic weights. In
		  addition, the binary neuron activation enables using the
		  sense amplifier instead of the analog-to-digital converter
		  to allow bitwise communication between layers of the neural
		  networks. However, the sense amplifier has intrinsic offset
		  that affects the threshold of binary neuron, thus it may
		  degrade the classification accuracy. In this work, we
		  analyze a fully parallel RRAM synaptic array architecture
		  that implements the fully connected layers in a
		  convolutional neural network with (+1, -1) weights and (+1,
		  0) neurons. The simulation results with TSMC 65 nm PDK show
		  that the offset of current mode sense amplifier introduces
		  a slight accuracy loss from ∼98.5{\%} to ∼97.6{\%} for
		  MNIST dataset. Nevertheless, the proposed fully parallel
		  BNN architecture (P-BNN) can achieve 137.35 TOPS/W energy
		  efficiency for the inference, improved by ∼20X compared
		  to the sequential BNN architecture (S-BNN) with row-by-row
		  read-out scheme. Moreover, the proposed P-BNN architecture
		  can save the chip area by ∼16{\%} as it eliminates the
		  area overhead of MAC peripheral units in the S-BNN
		  architecture.},
  author	= {Sun, Xiaoyu and Peng, Xiaochen and Chen, Pai Yu and Liu,
		  Rui and Seo, Jae Sun and Yu, Shimeng},
  booktitle	= {Proc. Asia South Pacific Des. Autom. Conf. ASP-DAC},
  doi		= {10.1109/ASPDAC.2018.8297384},
  isbn		= {9781509006021},
  month		= feb,
  pages		= {574--579},
  publisher	= {Institute of Electrical and Electronics Engineers Inc.},
  title		= {{Fully parallel RRAM synaptic array for implementing
		  binary neural network with (+1, -1) weights and (+1, 0)
		  neurons}},
  volume	= {2018-Janua},
  year		= {2018}
}
