
@Article{	  Berge2018,
  abstract	= {Medical applications challenge today's text categorization
		  techniques by demanding both high accuracy and
		  ease-of-interpretation. Although deep learning has provided
		  a leap ahead in accuracy, this leap comes at the sacrifice
		  of interpretability. To address this
		  accuracy-interpretability challenge, we here introduce, for
		  the first time, a text categorization approach that
		  leverages the recently introduced Tsetlin Machine. In all
		  brevity, we represent the terms of a text as propositional
		  variables. From these, we capture categories using simple
		  propositional formulae, such as: if "rash" and "reaction"
		  and "penicillin" then Allergy. The Tsetlin Machine learns
		  these formulae from a labelled text, utilizing conjunctive
		  clauses to represent the particular facets of each
		  category. Indeed, even the absence of terms (negated
		  features) can be used for categorization purposes. Our
		  empirical comparison with Na$\backslash$"ive Bayes,
		  decision trees, linear support vector machines (SVMs),
		  random forest, long short-term memory (LSTM) neural
		  networks, and other techniques, is quite conclusive. The
		  Tsetlin Machine either performs on par with or outperforms
		  all of the evaluated methods on both the 20 Newsgroups and
		  IMDb datasets, as well as on a non-public clinical dataset.
		  On average, the Tsetlin Machine delivers the best recall
		  and precision scores across the datasets. Finally, our GPU
		  implementation of the Tsetlin Machine executes 5 to 15
		  times faster than the CPU implementation, depending on the
		  dataset. We thus believe that our novel approach can have a
		  significant impact on a wide range of text analysis
		  applications, forming a promising starting point for deeper
		  natural language understanding with the Tsetlin Machine.},
  archiveprefix	= {arXiv},
  arxivid	= {1809.04547},
  author	= {Berge, Geir Thore and Granmo, Ole-Christoffer and Tveit,
		  Tor Oddbj{\o}rn and Goodwin, Morten and Jiao, Lei and
		  Matheussen, Bernt Viggo},
  eprint	= {1809.04547},
  month		= sep,
  title		= {{Using the Tsetlin Machine to Learn Human-Interpretable
		  Rules for High-Accuracy Text Categorization with Medical
		  Applications}},
  url		= {http://arxiv.org/abs/1809.04547},
  year		= {2018}
}

@Article{	  Constantinides2019,
  abstract	= {We consider efficiency in deep neural networks. Hardware
		  accelerators are gaining interest as machine learning
		  becomes one of the drivers of high-performance computing.
		  In these accelerators, the directed graph describing a
		  neural network can be implemented as a directed graph
		  describing a Boolean circuit. We make this observation
		  precise, leading naturally to an understanding of practical
		  neural networks as discrete functions, and show that
		  so-called binarised neural networks are functionally
		  complete. In general, our results suggest that it is
		  valuable to consider Boolean circuits as neural networks,
		  leading to the question of which circuit topologies are
		  promising. We argue that continuity is central to
		  generalisation in learning, explore the interaction between
		  data coding, network topology, and node functionality for
		  continuity, and pose some open questions for future
		  research. As a first step to bridging the gap between
		  continuous and Boolean views of neural network
		  accelerators, we present some recent results from our work
		  on LUTNet, a novel Field-Programmable Gate Array inference
		  approach. Finally, we conclude with additional possible
		  fruitful avenues for research bridging the continuous and
		  discrete views of neural networks.},
  archiveprefix	= {arXiv},
  arxivid	= {1905.02438},
  author	= {Constantinides, George A.},
  eprint	= {1905.02438},
  month		= may,
  title		= {{Rethinking Arithmetic for Deep Neural Networks}},
  url		= {http://arxiv.org/abs/1905.02438},
  year		= {2019}
}

@Article{	  Granmo2018,
  abstract	= {Although simple individually, artificial neurons provide
		  state-of-the-art performance when interconnected in deep
		  networks. Unknown to many, there exists an arguably even
		  simpler and more versatile learning mechanism, namely, the
		  Tsetlin Automaton. Merely by means of a single integer as
		  memory, it learns the optimal action in stochastic
		  environments through increment and decrement operations. In
		  this paper, we introduce the Tsetlin Machine, which solves
		  complex pattern recognition problems with easy-to-interpret
		  propositional formulas, composed by a collective of Tsetlin
		  Automata. To eliminate the longstanding problem of
		  vanishing signal-to-noise ratio, the Tsetlin Machine
		  orchestrates the automata using a novel game. Our
		  theoretical analysis establishes that the Nash equilibria
		  of the game align with the propositional formulas that
		  provide optimal pattern recognition accuracy. This
		  translates to learning without local optima, only global
		  ones. We argue that the Tsetlin Machine finds the
		  propositional formula that provides optimal accuracy, with
		  probability arbitrarily close to unity. In five benchmarks,
		  the Tsetlin Machine provides competitive accuracy compared
		  with SVMs, Decision Trees, Random Forests, Naive Bayes
		  Classifier, Logistic Regression, and Neural Networks. The
		  Tsetlin Machine further has an inherent computational
		  advantage since both inputs, patterns, and outputs are
		  expressed as bits, while recognition and learning rely on
		  bit manipulation. The combination of accuracy,
		  interpretability, and computational simplicity makes the
		  Tsetlin Machine a promising tool for a wide range of
		  domains. Being the first of its kind, we believe the
		  Tsetlin Machine will kick-start new paths of research, with
		  a potentially significant impact on the AI field and the
		  applications of AI.},
  archiveprefix	= {arXiv},
  arxivid	= {1804.01508},
  author	= {Granmo, Ole-Christoffer},
  eprint	= {1804.01508},
  month		= apr,
  title		= {{The Tsetlin Machine - A Game Theoretic Bandit Driven
		  Approach to Optimal Pattern Recognition with Propositional
		  Logic}},
  url		= {http://arxiv.org/abs/1804.01508},
  year		= {2018}
}

@TechReport{	  Granmo2019,
  author	= {Granmo, Ole-Christoffer},
  title		= {{Introduction to the Tsetlin Machine}},
  year		= {2019}
}

@Article{	  Narendra1974,
  author	= {Narendra, Kumpati S. and Thathachar, M. A. L.},
  doi		= {10.1109/TSMC.1974.5408453},
  issn		= {0018-9472},
  journal	= {IEEE Trans. Syst. Man. Cybern.},
  month		= jul,
  number	= {4},
  pages		= {323--334},
  title		= {{Learning Automata - A Survey}},
  url		= {http://ieeexplore.ieee.org/document/5408453/},
  volume	= {SMC-4},
  year		= {1974}
}

@TechReport{	  Tsetlin1961,
  author	= {Цетлин, М Л},
  institution	= {Avtomat. i Telemekh},
  title		= {{О ПОВЕДЕНИИ КОНЕЧНЫХ АВТОМАТОВ
		  В С ЛУЗИНЫХ СРЕДАХ}},
  url		= {http://www.mathnet.ru/links/ac3fad6dea1175b681335e87d3dff1a0/at12417.pdf},
  year		= {1961}
}

@Book{		  Tsetlin1973,
  author	= {Tsetlin, M. L.},
  isbn		= {9780080956114},
  publisher	= {Academic Press},
  title		= {{Automaton Theory and Modeling of Biological Systems}},
  year		= {1973}
}
