
@Article{	  Constantinides2019,
  abstract	= {We consider efficiency in deep neural networks. Hardware
		  accelerators are gaining interest as machine learning
		  becomes one of the drivers of high-performance computing.
		  In these accelerators, the directed graph describing a
		  neural network can be implemented as a directed graph
		  describing a Boolean circuit. We make this observation
		  precise, leading naturally to an understanding of practical
		  neural networks as discrete functions, and show that
		  so-called binarised neural networks are functionally
		  complete. In general, our results suggest that it is
		  valuable to consider Boolean circuits as neural networks,
		  leading to the question of which circuit topologies are
		  promising. We argue that continuity is central to
		  generalisation in learning, explore the interaction between
		  data coding, network topology, and node functionality for
		  continuity, and pose some open questions for future
		  research. As a first step to bridging the gap between
		  continuous and Boolean views of neural network
		  accelerators, we present some recent results from our work
		  on LUTNet, a novel Field-Programmable Gate Array inference
		  approach. Finally, we conclude with additional possible
		  fruitful avenues for research bridging the continuous and
		  discrete views of neural networks.},
  archiveprefix	= {arXiv},
  arxivid	= {1905.02438},
  author	= {Constantinides, George A.},
  eprint	= {1905.02438},
  month		= may,
  title		= {{Rethinking Arithmetic for Deep Neural Networks}},
  url		= {http://arxiv.org/abs/1905.02438},
  year		= {2019}
}

@Article{	  Granmo2018,
  abstract	= {Although simple individually, artificial neurons provide
		  state-of-the-art performance when interconnected in deep
		  networks. Unknown to many, there exists an arguably even
		  simpler and more versatile learning mechanism, namely, the
		  Tsetlin Automaton. Merely by means of a single integer as
		  memory, it learns the optimal action in stochastic
		  environments through increment and decrement operations. In
		  this paper, we introduce the Tsetlin Machine, which solves
		  complex pattern recognition problems with easy-to-interpret
		  propositional formulas, composed by a collective of Tsetlin
		  Automata. To eliminate the longstanding problem of
		  vanishing signal-to-noise ratio, the Tsetlin Machine
		  orchestrates the automata using a novel game. Our
		  theoretical analysis establishes that the Nash equilibria
		  of the game align with the propositional formulas that
		  provide optimal pattern recognition accuracy. This
		  translates to learning without local optima, only global
		  ones. We argue that the Tsetlin Machine finds the
		  propositional formula that provides optimal accuracy, with
		  probability arbitrarily close to unity. In five benchmarks,
		  the Tsetlin Machine provides competitive accuracy compared
		  with SVMs, Decision Trees, Random Forests, Naive Bayes
		  Classifier, Logistic Regression, and Neural Networks. The
		  Tsetlin Machine further has an inherent computational
		  advantage since both inputs, patterns, and outputs are
		  expressed as bits, while recognition and learning rely on
		  bit manipulation. The combination of accuracy,
		  interpretability, and computational simplicity makes the
		  Tsetlin Machine a promising tool for a wide range of
		  domains. Being the first of its kind, we believe the
		  Tsetlin Machine will kick-start new paths of research, with
		  a potentially significant impact on the AI field and the
		  applications of AI.},
  archiveprefix	= {arXiv},
  arxivid	= {1804.01508},
  author	= {Granmo, Ole-Christoffer},
  eprint	= {1804.01508},
  month		= apr,
  title		= {{The Tsetlin Machine - A Game Theoretic Bandit Driven
		  Approach to Optimal Pattern Recognition with Propositional
		  Logic}},
  url		= {http://arxiv.org/abs/1804.01508},
  year		= {2018}
}
