
@InProceedings{	  Ahad,
  author	= {Ahad, A. and Fayyaz, A. and Mehmood, T.},
  booktitle	= {IEEE Students Conf. ISCON '02. Proceedings.},
  doi		= {10.1109/ISCON.2002.1215948},
  isbn		= {0-7803-7505-X},
  pages		= {103--109},
  publisher	= {IEEE},
  title		= {{Speech recognition using multilayer perceptron}},
  url		= {http://ieeexplore.ieee.org/document/1215948/},
  volume	= {1}
}

@Article{	  Burr1987,
  author	= {Burr, D.J.},
  journal	= {Proc. 1987 Int. Conf. Neural Inf. Process. Syst.},
  pages		= {144--153},
  title		= {{Speech recognition experiments with perceptrons}},
  url		= {https://dl.acm.org/citation.cfm?id=2969659},
  year		= {1987}
}

@Article{	  Sarwar2016,
  abstract	= {Large-scale artificial neural networks have shown
		  significant promise in addressing a wide range of
		  classification and recognition applications. However, their
		  large computational requirements stretch the capabilities
		  of computing platforms. The fundamental components of these
		  neural networks are the neurons and its synapses. The core
		  of a digital hardware neuron consists of multiplier,
		  accumulator and activation function. Multipliers consume
		  most of the processing energy in the digital neurons, and
		  thereby in the hardware implementations of artificial
		  neural networks. We propose an approximate multiplier that
		  utilizes the notion of computation sharing and exploits
		  error resilience of neural network applications to achieve
		  improved energy consumption. We also propose
		  Multiplier-less Artificial Neuron (MAN) for even larger
		  improvement in energy consumption and adapt the training
		  process to ensure minimal degradation in accuracy. We
		  evaluated the proposed design on 5 recognition
		  applications. The results show, 35{\%} and 60{\%} reduction
		  in energy consumption, for neuron sizes of 8 bits and 12
		  bits, respectively, with a maximum of {\~{}}2.83{\%} loss
		  in network accuracy, compared to a conventional neuron
		  implementation. We also achieve 37{\%} and 62{\%} reduction
		  in area for a neuron size of 8 bits and 12 bits,
		  respectively, under iso-speed conditions.},
  archiveprefix	= {arXiv},
  arxivid	= {1602.08557},
  author	= {Sarwar, Syed Shakib and Venkataramani, Swagath and
		  Raghunathan, Anand and Roy, Kaushik},
  eprint	= {1602.08557},
  isbn		= {9783981537062},
  journal	= {Date 16},
  keywords	= {alphabet set multiplier,ann,artificial
		  neural,asm,computation sharing
		  multiplication,cshm,man,multiplier-less artificial
		  neuron,network},
  title		= {{Multiplier-less Artificial Neurons Exploiting Error
		  Resiliency for Energy-Efficient Neural Computing}},
  year		= {2016}
}
