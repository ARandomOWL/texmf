
@InProceedings{	  Ahad,
  author	= {Ahad, A. and Fayyaz, A. and Mehmood, T.},
  booktitle	= {IEEE Students Conf. ISCON '02. Proceedings.},
  doi		= {10.1109/ISCON.2002.1215948},
  isbn		= {0-7803-7505-X},
  pages		= {103--109},
  publisher	= {IEEE},
  title		= {{Speech recognition using multilayer perceptron}},
  url		= {http://ieeexplore.ieee.org/document/1215948/},
  volume	= {1}
}

@Misc{		  AniketBadhan,
  author	= {AniketBadhan},
  title		= {{Convolutional-Neural-Network (Verilog)}},
  url		= {https://github.com/AniketBadhan/Convolutional-Neural-Network}
}

@Misc{		  Bittware,
  author	= {Bittware},
  title		= {{FPGA Acceleration of Convolutional Neural Networks}},
  url		= {https://www.bittware.com/resources/cnn/}
}

@Article{	  Burr1987,
  author	= {Burr, D.J.},
  journal	= {Proc. 1987 Int. Conf. Neural Inf. Process. Syst.},
  pages		= {144--153},
  title		= {{Speech recognition experiments with perceptrons}},
  url		= {https://dl.acm.org/citation.cfm?id=2969659},
  year		= {1987}
}

@Article{	  Courbariaux2016,
  abstract	= {We introduce a method to train Binarized Neural Networks
		  (BNNs) - neural networks with binary weights and
		  activations at run-time. At training-time the binary
		  weights and activations are used for computing the
		  parameters gradients. During the forward pass, BNNs
		  drastically reduce memory size and accesses, and replace
		  most arithmetic operations with bit-wise operations, which
		  is expected to substantially improve power-efficiency. To
		  validate the effectiveness of BNNs we conduct two sets of
		  experiments on the Torch7 and Theano frameworks. On both,
		  BNNs achieved nearly state-of-the-art results over the
		  MNIST, CIFAR-10 and SVHN datasets. Last but not least, we
		  wrote a binary matrix multiplication GPU kernel with which
		  it is possible to run our MNIST BNN 7 times faster than
		  with an unoptimized GPU kernel, without suffering any loss
		  in classification accuracy. The code for training and
		  running our BNNs is available on-line.},
  archiveprefix	= {arXiv},
  arxivid	= {1602.02830},
  author	= {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel
		  and El-Yaniv, Ran and Bengio, Yoshua},
  eprint	= {1602.02830},
  month		= feb,
  title		= {{Binarized Neural Networks: Training Deep Neural Networks
		  with Weights and Activations Constrained to +1 or -1}},
  url		= {http://arxiv.org/abs/1602.02830},
  year		= {2016}
}

@Article{	  Courbariaux2016a,
  abstract	= {We introduce a method to train Binarized Neural Networks
		  (BNNs) - neural networks with binary weights and
		  activations at run-time. At training-time the binary
		  weights and activations are used for computing the
		  parameters gradients. During the forward pass, BNNs
		  drastically reduce memory size and accesses, and replace
		  most arithmetic operations with bit-wise operations, which
		  is expected to substantially improve power-efficiency. To
		  validate the effectiveness of BNNs we conduct two sets of
		  experiments on the Torch7 and Theano frameworks. On both,
		  BNNs achieved nearly state-of-the-art results over the
		  MNIST, CIFAR-10 and SVHN datasets. Last but not least, we
		  wrote a binary matrix multiplication GPU kernel with which
		  it is possible to run our MNIST BNN 7 times faster than
		  with an unoptimized GPU kernel, without suffering any loss
		  in classification accuracy. The code for training and
		  running our BNNs is available on-line.},
  archiveprefix	= {arXiv},
  arxivid	= {1602.02830},
  author	= {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel
		  and El-Yaniv, Ran and Bengio, Yoshua},
  eprint	= {1602.02830},
  month		= feb,
  title		= {{Binarized Neural Networks: Training Deep Neural Networks
		  with Weights and Activations Constrained to +1 or -1}},
  url		= {http://arxiv.org/abs/1602.02830},
  year		= {2016}
}

@InProceedings{	  Ding2018,
  abstract	= {Deep Neural Networks (DNNs) have been adopted in many
		  systems because of their higher classification accuracy,
		  with custom hardware implementations great candidates for
		  high-speed, accurate inference. While progress in achieving
		  large scale, highly accurate DNNs has been made,
		  significant energy and area are required due to massive
		  memory accesses and computations. Such demands pose a
		  challenge to any DNN implementation, yet it is more natural
		  to handle in a custom hardware platform. To alleviate the
		  increased demand in storage and energy, quantized DNNs
		  constrain their weights (and activations) from
		  floating-point numbers to only a few discrete levels.
		  Therefore, storage is reduced, thereby leading to less
		  memory accesses. In this paper, we provide an overview of
		  different types of quantized DNNs, as well as the training
		  approaches for them. Among the various quantized DNNs, our
		  LightNN (Light Neural Network) approach can reduce both
		  memory accesses and computation energy, by filling the gap
		  between classic, full-precision and binarized DNNs. We
		  provide a detailed comparison between LightNNs,
		  conventional DNNs and Binarized Neural Networks (BNNs),
		  with MNIST and CIFAR-10 datasets. In contrast to other
		  quantized DNNs that trade-off significant amounts of
		  accuracy for lower memory requirements, LightNNs can
		  significantly reduce storage, energy and area while still
		  maintaining a test error similar to a large DNN
		  configuration. Thus, LightNNs provide more options for
		  hardware designers to trade-off accuracy and energy.
		  {\textcopyright} 2018 IEEE.},
  author	= {Ding, Ruizhou and Liu, Zeye and Blanton, R. D.Shawn and
		  Marculescu, Diana},
  booktitle	= {Proc. Asia South Pacific Des. Autom. Conf. ASP-DAC},
  doi		= {10.1109/ASPDAC.2018.8297274},
  isbn		= {9781509006021},
  month		= jan,
  pages		= {1--8},
  publisher	= {IEEE},
  title		= {{Quantized deep neural networks for energy efficient
		  hardware-based inference}},
  url		= {http://ieeexplore.ieee.org/document/8297274/},
  volume	= {2018-Janua},
  year		= {2018}
}

@Article{	  Fick2018,
  author	= {Fick, Dave and Henry, Mike},
  journal	= {Hot Chips 30},
  title		= {{Analog Computation in Flash Memory for Datacenter-scale
		  AI Inference in a Small Chip}},
  year		= {2018}
}

@Article{	  Hopfield1998,
  abstract	= {Most computational engineering based loosely on biology
		  uses continuous variables to represent neural activity. Yet
		  most neurons communicate with action potentials. The
		  engineering view is equivalent to using a rate-code for
		  representing information and for computing. An increasing
		  number of examples are being discovered in which biology
		  may not be using rate codes. Information can be represented
		  using the timing of action potentials, and efficiently
		  computed with in this representation. The "analog match"
		  problem of odour identification is a simple problem which
		  can be efficiently solved using action potential timing and
		  an underlying rhythm. By using adapting units to effect a
		  fundamental change of representation of a problem, we map
		  the recognition of words (having uniform time-warp) in
		  connected speech into the same analog match problem. We
		  describe the architecture and preliminary results of such a
		  recognition system. Using the fast events of biology in
		  conjunction with an underlying rhythm is one way to
		  overcome the limits of an event-driven view of computation.
		  When the intrinsic hardware is much faster than the time
		  scale of change of inputs, this approach can greatly
		  increase the effective computation per unit time on a given
		  quantity of hardware.},
  author	= {Hopfield, J J and Carlos, C D and Roweis, S},
  isbn		= {1049-5258},
  issn		= {10495258},
  journal	= {Adv. Neural Inf. Process. Syst. 10},
  pages		= {166--172},
  title		= {{Computing with action potentials}},
  volume	= {10},
  year		= {1998}
}

@Article{	  Johnson2018,
  abstract	= {Reducing hardware overhead of neural networks for faster
		  or lower power inference and training is an active area of
		  research. Uniform quantization using integer multiply-add
		  has been thoroughly investigated, which requires learning
		  many quantization parameters, fine-tuning training or other
		  prerequisites. Little effort is made to improve floating
		  point relative to this baseline; it remains energy
		  inefficient, and word size reduction yields drastic loss in
		  needed dynamic range. We improve floating point to be more
		  energy efficient than equivalent bit width integer hardware
		  on a 28 nm ASIC process while retaining accuracy in 8 bits
		  with a novel hybrid log multiply/linear add, Kulisch
		  accumulation and tapered encodings from Gustafson's posit
		  format. With no network retraining, and drop-in replacement
		  of all math and float32 parameters via
		  round-to-nearest-even only, this open-sourced 8-bit log
		  float is within 0.9{\%} top-1 and 0.2{\%} top-5 accuracy of
		  the original float32 ResNet-50 CNN model on ImageNet.
		  Unlike int8 quantization, it is still a general purpose
		  floating point arithmetic, interpretable out-of-the-box.
		  Our 8/38-bit log float multiply-add is synthesized and
		  power profiled at 28 nm at 0.96x the power and 1.12x the
		  area of 8/32-bit integer multiply-add. In 16 bits, our log
		  float multiply-add is 0.59x the power and 0.68x the area of
		  IEEE 754 float16 fused multiply-add, maintaining the same
		  signficand precision and dynamic range, proving useful for
		  training ASICs as well.},
  archiveprefix	= {arXiv},
  arxivid	= {1811.01721},
  author	= {Johnson, Jeff},
  eprint	= {1811.01721},
  month		= nov,
  title		= {{Rethinking floating point for deep learning}},
  url		= {http://arxiv.org/abs/1811.01721},
  year		= {2018}
}

@InProceedings{	  Lin2016,
  author	= {Lin, Yingyan and Zhang, Sai and Shanbhag, Naresh R.},
  booktitle	= {2016 IEEE Int. Work. Signal Process. Syst.},
  doi		= {10.1109/SiPS.2016.11},
  isbn		= {978-1-5090-3361-4},
  month		= oct,
  pages		= {17--22},
  publisher	= {IEEE},
  title		= {{Variation-Tolerant Architectures for Convolutional Neural
		  Networks in the Near Threshold Voltage Regime}},
  url		= {http://ieeexplore.ieee.org/document/7780065/},
  year		= {2016}
}

@Book{		  Minsky1969,
  abstract	= {I. Algebraic theory of linear parallel predicates -- 1.
		  Theory of linear Boolean inequalities -- 2. Group
		  invariance of Boolean inequalities -- 3. Parity and
		  one-in-a-box predicates -- 4. The "and/or" theorem -- II.
		  Geometric theory of linear inequalities -- 5.
		  $\Psi$connected: a geometric property with unbounded order
		  -- 6. Geometric patterns of small order: spectra and
		  context -- 7. Stratification and normalization -- 8. The
		  diameter-limited perceptron -- 9. Geometric predicates and
		  serial algorithms -- III. Learning theory -- 10. Magnitude
		  of the coefficients -- 11. Learning -- 12. Linear
		  separation and learning -- 13. Perceptrons and pattern
		  recognition.},
  author	= {Minsky, Marvin and Papert, Seymour},
  isbn		= {9780262130431},
  pages		= {258},
  publisher	= {MIT Press},
  title		= {{Perceptrons; an introduction to computational geometry}},
  year		= {1969}
}

@Article{	  Sarwar2016,
  abstract	= {Large-scale artificial neural networks have shown
		  significant promise in addressing a wide range of
		  classification and recognition applications. However, their
		  large computational requirements stretch the capabilities
		  of computing platforms. The fundamental components of these
		  neural networks are the neurons and its synapses. The core
		  of a digital hardware neuron consists of multiplier,
		  accumulator and activation function. Multipliers consume
		  most of the processing energy in the digital neurons, and
		  thereby in the hardware implementations of artificial
		  neural networks. We propose an approximate multiplier that
		  utilizes the notion of computation sharing and exploits
		  error resilience of neural network applications to achieve
		  improved energy consumption. We also propose
		  Multiplier-less Artificial Neuron (MAN) for even larger
		  improvement in energy consumption and adapt the training
		  process to ensure minimal degradation in accuracy. We
		  evaluated the proposed design on 5 recognition
		  applications. The results show, 35{\%} and 60{\%} reduction
		  in energy consumption, for neuron sizes of 8 bits and 12
		  bits, respectively, with a maximum of {\~{}}2.83{\%} loss
		  in network accuracy, compared to a conventional neuron
		  implementation. We also achieve 37{\%} and 62{\%} reduction
		  in area for a neuron size of 8 bits and 12 bits,
		  respectively, under iso-speed conditions.},
  archiveprefix	= {arXiv},
  arxivid	= {1602.08557},
  author	= {Sarwar, Syed Shakib and Venkataramani, Swagath and
		  Raghunathan, Anand and Roy, Kaushik},
  eprint	= {1602.08557},
  isbn		= {9783981537062},
  journal	= {Date 16},
  keywords	= {alphabet set multiplier,ann,artificial
		  neural,asm,computation sharing
		  multiplication,cshm,man,multiplier-less artificial
		  neuron,network},
  month		= feb,
  title		= {{Multiplier-less Artificial Neurons Exploiting Error
		  Resiliency for Energy-Efficient Neural Computing}},
  url		= {http://arxiv.org/abs/1602.08557},
  year		= {2016}
}

@Article{	  Sarwar2018,
  author	= {Sarwar, Syed Shakib and Srinivasan, Gopalakrishnan and
		  Han, Bing and Wijesinghe, Parami and Jaiswal, Akhilesh and
		  Panda, Priyadarshini and Raghunathan, Anand and Roy,
		  Kaushik},
  doi		= {10.1109/JETCAS.2018.2835809},
  issn		= {2156-3357},
  journal	= {IEEE J. Emerg. Sel. Top. Circuits Syst.},
  pages		= {1--1},
  title		= {{Energy Efficient Neural Computing: A Study of Cross-Layer
		  Approximations}},
  url		= {https://ieeexplore.ieee.org/document/8358698/},
  year		= {2018}
}

@InProceedings{	  Tang1997,
  author	= {Tang, C.Z. and Kwan, H.K.},
  booktitle	= {Proc. 1997 IEEE Int. Symp. Circuits Syst. Circuits Syst.
		  Inf. Age ISCAS '97},
  doi		= {10.1109/ISCAS.1997.608912},
  isbn		= {0-7803-3583-X},
  pages		= {649--652},
  publisher	= {IEEE},
  title		= {{Digital implementation of neural networks with quantized
		  neurons}},
  url		= {http://ieeexplore.ieee.org/document/608912/},
  volume	= {1},
  year		= {1997}
}

@InProceedings{	  Venkataramani2014,
  abstract	= {Neuromorphic algorithms, which are comprised of highly
		  complex, large-scale networks of artificial neurons, are
		  increasingly used for a variety of recognition,
		  classification, search and vision tasks. However, their
		  computational and energy requirements can be quite high,
		  and hence their energy-efficient implementation is of great
		  interest. We propose a new approach to design
		  energy-efficient hardware implementations of large-scale
		  neural networks (NNs) using approximate computing. Our work
		  is motivated by the observations that (i) NNs are used in
		  applications where less-than-perfect results are
		  acceptable, and often inevitable, and (ii) they are highly
		  resilient to inexactness in many (but not all) of their
		  constituent computations. We make two key contributions.
		  First, we propose a method to transform any given NN into
		  an Approximate Neural Network (AxNN). This is performed by
		  (i) adapting the backpropagation technique, which is
		  commonly used to train these networks, to quantify the
		  impact of approximating each neuron to the overall network
		  quality (e.g., classification accuracy), and (ii)
		  selectively approximating those neurons that impact network
		  quality the least. Further, we make the key observation
		  that training is a naturally error-healing process that can
		  be used to mitigate the impact of approximations to
		  neurons. Therefore, we incrementally retrain the network
		  with the approximations in-place, reclaiming a significant
		  portion of the quality ceded by approximations. As a second
		  contribution, we propose a programmable and
		  quality-configurable neuromorphic processing engine
		  (qcNPE), which utilizes arrays of specialized processing
		  elements that execute neuron computations with dynamically
		  configurable accuracies and can be used to execute AxNNs
		  from diverse applications. We evaluated the proposed
		  approach by constructing AXNNs for 6 recognition
		  applications (ranging in complexity from 12-47,818 neurons
		  and 160-3,155,968 connections) and executing them on two
		  different platforms-qcNPE implementation containing 272
		  processing elements in 45nm technology and a commodity
		  Intel Xeon server. Our results demonstrate 1.14X-1.92X
		  energy benefits for virtually no loss ({\textless} 0.5{\%})
		  in output quality, and even higher improvements (upto 2.3X)
		  when some loss (upto 7.5{\%}) in output quality is
		  acceptable.},
  address	= {New York, New York, USA},
  author	= {Venkataramani, Swagath and Ranjan, Ashish and Roy, Kaushik
		  and Raghunathan, Anand},
  booktitle	= {Proc. 2014 Int. Symp. Low power Electron. Des. - ISLPED
		  '14},
  doi		= {10.1145/2627369.2627613},
  isbn		= {9781450329750},
  pages		= {27--32},
  publisher	= {ACM Press},
  title		= {{AxNN: Energy-efficient neuromorphic systems using
		  approximate computing}},
  url		= {http://dl.acm.org/citation.cfm?doid=2627369.2627613},
  year		= {2014}
}

@Misc{		  Wang,
  abstract	= {PipeCNN is an OpenCL-based FPGA Accelerator for
		  Large-Scale Convolutional Neural Networks (CNNs).},
  author	= {Wang, Dong},
  title		= {{PipeCNN}},
  url		= {https://github.com/doonny/PipeCNN}
}

@Article{	  Zhang2018,
  abstract	= {Hardware accelerators are being increasingly deployed to
		  boost the performance and energy efficiency of deep neural
		  network (DNN) inference. In this paper we propose
		  Thundervolt, a new framework that enables aggressive
		  voltage underscaling of high-performance DNN accelerators
		  without compromising classification accuracy even in the
		  presence of high timing error rates. Using post-synthesis
		  timing simulations of a DNN accelerator modeled on the
		  Google TPU, we show that Thundervolt enables between
		  34{\%}-57{\%} energy savings on state-of-the-art speech and
		  image recognition benchmarks with less than 1{\%} loss in
		  classification accuracy and no performance loss. Further,
		  we show that Thundervolt is synergistic with and can
		  further increase the energy efficiency of commonly used
		  run-time DNN pruning techniques like Zero-Skip.},
  archiveprefix	= {arXiv},
  arxivid	= {1802.03806},
  author	= {Zhang, Jeff and Rangineni, Kartheek and Ghodsi, Zahra and
		  Garg, Siddharth},
  eprint	= {1802.03806},
  month		= feb,
  title		= {{ThUnderVolt: Enabling Aggressive Voltage Underscaling and
		  Timing Error Resilience for Energy Efficient Deep Neural
		  Network Accelerators}},
  url		= {http://arxiv.org/abs/1802.03806},
  year		= {2018}
}
