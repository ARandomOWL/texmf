
@InProceedings{	  Ahad,
  author	= {Ahad, A. and Fayyaz, A. and Mehmood, T.},
  booktitle	= {IEEE Students Conf. ISCON '02. Proceedings.},
  doi		= {10.1109/ISCON.2002.1215948},
  isbn		= {0-7803-7505-X},
  pages		= {103--109},
  publisher	= {IEEE},
  title		= {{Speech recognition using multilayer perceptron}},
  url		= {http://ieeexplore.ieee.org/document/1215948/},
  volume	= {1}
}

@Article{	  Burr1987,
  author	= {Burr, D.J.},
  journal	= {Proc. 1987 Int. Conf. Neural Inf. Process. Syst.},
  pages		= {144--153},
  title		= {{Speech recognition experiments with perceptrons}},
  url		= {https://dl.acm.org/citation.cfm?id=2969659},
  year		= {1987}
}

@Article{	  Hopfield1998,
  abstract	= {Most computational engineering based loosely on biology
		  uses continuous variables to represent neural activity. Yet
		  most neurons communicate with action potentials. The
		  engineering view is equivalent to using a rate-code for
		  representing information and for computing. An increasing
		  number of examples are being discovered in which biology
		  may not be using rate codes. Information can be represented
		  using the timing of action potentials, and efficiently
		  computed with in this representation. The "analog match"
		  problem of odour identification is a simple problem which
		  can be efficiently solved using action potential timing and
		  an underlying rhythm. By using adapting units to effect a
		  fundamental change of representation of a problem, we map
		  the recognition of words (having uniform time-warp) in
		  connected speech into the same analog match problem. We
		  describe the architecture and preliminary results of such a
		  recognition system. Using the fast events of biology in
		  conjunction with an underlying rhythm is one way to
		  overcome the limits of an event-driven view of computation.
		  When the intrinsic hardware is much faster than the time
		  scale of change of inputs, this approach can greatly
		  increase the effective computation per unit time on a given
		  quantity of hardware.},
  author	= {Hopfield, J J and Carlos, C D and Roweis, S},
  isbn		= {1049-5258},
  issn		= {10495258},
  journal	= {Adv. Neural Inf. Process. Syst. 10},
  pages		= {166--172},
  title		= {{Computing with action potentials}},
  volume	= {10},
  year		= {1998}
}

@InProceedings{	  Lin2016,
  author	= {Lin, Yingyan and Zhang, Sai and Shanbhag, Naresh R.},
  booktitle	= {2016 IEEE Int. Work. Signal Process. Syst.},
  doi		= {10.1109/SiPS.2016.11},
  isbn		= {978-1-5090-3361-4},
  month		= oct,
  pages		= {17--22},
  publisher	= {IEEE},
  title		= {{Variation-Tolerant Architectures for Convolutional Neural
		  Networks in the Near Threshold Voltage Regime}},
  url		= {http://ieeexplore.ieee.org/document/7780065/},
  year		= {2016}
}

@Article{	  Sarwar2016,
  abstract	= {Large-scale artificial neural networks have shown
		  significant promise in addressing a wide range of
		  classification and recognition applications. However, their
		  large computational requirements stretch the capabilities
		  of computing platforms. The fundamental components of these
		  neural networks are the neurons and its synapses. The core
		  of a digital hardware neuron consists of multiplier,
		  accumulator and activation function. Multipliers consume
		  most of the processing energy in the digital neurons, and
		  thereby in the hardware implementations of artificial
		  neural networks. We propose an approximate multiplier that
		  utilizes the notion of computation sharing and exploits
		  error resilience of neural network applications to achieve
		  improved energy consumption. We also propose
		  Multiplier-less Artificial Neuron (MAN) for even larger
		  improvement in energy consumption and adapt the training
		  process to ensure minimal degradation in accuracy. We
		  evaluated the proposed design on 5 recognition
		  applications. The results show, 35{\%} and 60{\%} reduction
		  in energy consumption, for neuron sizes of 8 bits and 12
		  bits, respectively, with a maximum of {\~{}}2.83{\%} loss
		  in network accuracy, compared to a conventional neuron
		  implementation. We also achieve 37{\%} and 62{\%} reduction
		  in area for a neuron size of 8 bits and 12 bits,
		  respectively, under iso-speed conditions.},
  archiveprefix	= {arXiv},
  arxivid	= {1602.08557},
  author	= {Sarwar, Syed Shakib and Venkataramani, Swagath and
		  Raghunathan, Anand and Roy, Kaushik},
  eprint	= {1602.08557},
  isbn		= {9783981537062},
  journal	= {Date 16},
  keywords	= {alphabet set multiplier,ann,artificial
		  neural,asm,computation sharing
		  multiplication,cshm,man,multiplier-less artificial
		  neuron,network},
  title		= {{Multiplier-less Artificial Neurons Exploiting Error
		  Resiliency for Energy-Efficient Neural Computing}},
  year		= {2016}
}

@Article{	  Sarwar2018,
  author	= {Sarwar, Syed Shakib and Srinivasan, Gopalakrishnan and
		  Han, Bing and Wijesinghe, Parami and Jaiswal, Akhilesh and
		  Panda, Priyadarshini and Raghunathan, Anand and Roy,
		  Kaushik},
  doi		= {10.1109/JETCAS.2018.2835809},
  issn		= {2156-3357},
  journal	= {IEEE J. Emerg. Sel. Top. Circuits Syst.},
  pages		= {1--1},
  title		= {{Energy Efficient Neural Computing: A Study of Cross-Layer
		  Approximations}},
  url		= {https://ieeexplore.ieee.org/document/8358698/},
  year		= {2018}
}

@Article{	  Zhang2018,
  abstract	= {Hardware accelerators are being increasingly deployed to
		  boost the performance and energy efficiency of deep neural
		  network (DNN) inference. In this paper we propose
		  Thundervolt, a new framework that enables aggressive
		  voltage underscaling of high-performance DNN accelerators
		  without compromising classification accuracy even in the
		  presence of high timing error rates. Using post-synthesis
		  timing simulations of a DNN accelerator modeled on the
		  Google TPU, we show that Thundervolt enables between
		  34{\%}-57{\%} energy savings on state-of-the-art speech and
		  image recognition benchmarks with less than 1{\%} loss in
		  classification accuracy and no performance loss. Further,
		  we show that Thundervolt is synergistic with and can
		  further increase the energy efficiency of commonly used
		  run-time DNN pruning techniques like Zero-Skip.},
  archiveprefix	= {arXiv},
  arxivid	= {1802.03806},
  author	= {Zhang, Jeff and Rangineni, Kartheek and Ghodsi, Zahra and
		  Garg, Siddharth},
  eprint	= {1802.03806},
  month		= feb,
  title		= {{ThUnderVolt: Enabling Aggressive Voltage Underscaling and
		  Timing Error Resilience for Energy Efficient Deep Neural
		  Network Accelerators}},
  url		= {http://arxiv.org/abs/1802.03806},
  year		= {2018}
}
